[env_parameters]
num_jobs = 10           # Number of jobs in the environment
num_mas = 5             # Number of machine agents
batch_size = 20          # Batch size for training
ope_feat_dim = 6       # Dimension of operation features
ma_feat_dim = 3         # Dimension of machine agent features
valid_batch_size = 5    # Batch size for validation
show_mode = "print"     # Mode for displaying information (e.g., "print")
device = "cuda"              # Device for training ("cpu" or "cuda")

[model_parameters]
in_size_ma = 3              # Input size for machine agent
out_size_ma = 8             # Output size for machine agent
in_size_ope = 6           # Input size for operation
out_size_ope = 8            # Output size for operation
hidden_size_ope = 128       # Hidden size for operation model
num_heads = [1, 1]          # Number of attention heads
dropout = 0.0               # Dropout probability
n_latent_actor = 64         # Number of latent units for actor
n_latent_critic = 64        # Number of latent units for critic
n_hidden_actor = 3          # Number of hidden layers for actor
n_hidden_critic = 3         # Number of hidden layers for critic
action_dim = 1              # Dimension of action space
device = "cuda"              # Device for training ("cpu" or "cuda")

[train_parameters]
lr = 0.0002                 # Learning rate
betas = [0.9, 0.999]        # Beta values for Adam optimizer
gamma = 1.0                 # Discount factor
K_epochs = 3                # Number of update epochs
eps_clip = 0.2              # Epsilon for clipping in PPO
A_coeff = 1.0               # Coefficient for actor loss
vf_coeff = 0.5              # Coefficient for value function loss
entropy_coeff = 0.01        # Coefficient for entropy loss
max_iterations = 1000       # Maximum number of iterations
save_timestep = 20          # Timestep for saving model checkpoints
update_timestep = 1         # Timestep for model updates
viz = true                 # Visualize the training process --> when True: first launch local server with: python -m visdom.server
viz_name = 'training_visualisation' # Name of visualization env
minibatch_size = 512        # Mini-batch size for training
parallel_iter = 20          # Number of parallel iterations
device = "cuda"              # Device for training ("cpu" or "cuda")
validation_folder = "/fjsp/song/dev/1005/"  # Folder for validation data


# Configuration for test parameters
[test_parameters]
problem_instance = "/fjsp/1_Brandimarte/Mk02.fjs"               # Problem instance for testing
trained_policy = "./solutions/FJSP_DRL/save/train_20240314_192906/song_10_5.pt"          # Load pretrained policy
sample = true                                                  # Sampling flag for testing
num_sample = 5                                                  # Number of samples for testing (nr )
device = "cuda"                                                  # Device for testing ("cpu" or "cuda")